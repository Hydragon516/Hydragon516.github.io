@inproceedings{lee2021regularization,
  abbr={CVPR},
  title={Regularization strategy for point cloud via rigidly mixed sample},
  author={Lee, Dogyoon and Lee, Jaeha and Lee, Junhyeop and Lee, Hyeongmin and Lee, Minhyeok and Woo, Sungmin and Lee, Sangyoun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15900--15909},
  year={2021},
  arxiv={2102.01929},
  code={https://github.com/dogyoonlee/RSMix},
  abstract={Data augmentation is an effective regularization strategy to alleviate the overfitting, which is an inherent drawback of the deep neural networks. However, data augmentation is rarely considered for point cloud processing despite many studies proposing various augmentation methods for image data. Actually, regularization is essential for point clouds since lack of generality is more likely to occur in point cloud due to small datasets. This paper proposes a Rigid Subset Mix (RSMix), a novel data augmentation method for point clouds that generates a virtual mixed sample by replacing part of the sample with shape-preserved subsets from another sample. RSMix preserves structural information of the point cloud sample by extracting subsets from each sample without deformation using a neighboring function. The neighboring function was carefully designed considering unique properties of point cloud, unordered structure and non-grid. Experiments verified that RSMix successfully regularized the deep neural networks with remarkable improvement for shape classification. We also analyzed various combinations of data augmentations including RSMix with single and multi-view evaluations, based on abundant ablation studies.}
}

@inproceedings{lee2022robust,
  abbr={WACV},
  title={Robust lane detection via expanded self attention},
  author={Lee, Minhyeok and Lee, Junhyeop and Lee, Dogyoon and Kim, Woojin and Hwang, Sangwon and Lee, Sangyoun},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={533--542},
  year={2022},
  arxiv={2102.07037},
  code={https://github.com/Hydragon516/ESA-official},
  abstract={The image-based lane detection algorithm is one of the key technologies in autonomous vehicles. Modern deep learning methods achieve high performance in lane detection, but it is still difficult to accurately detect lanes in challenging situations such as congested roads and extreme lighting conditions. To be robust on these challenging situations, it is important to extract global contextual information even from limited visual cues. In this paper, we propose a simple but powerful self-attention mechanism optimized for lane detection called the Expanded Self Attention (ESA) module. Inspired by the simple geometric structure of lanes, the proposed method predicts the confidence of a lane along the vertical and horizontal directions in an image. The prediction of the confidence enables estimating occluded locations by extracting global contextual information. ESA module can be easily implemented and applied to any encoder-decoder-based model without increasing the inference time. The performance of our method is evaluated on three popular lane detection benchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art performance in CULane and BDD100K and distinct improvement on TuSimple dataset. The experimental results show that our approach is robust to occlusion and extreme lighting conditions.}
}

@inproceedings{lee2022spsn,
  abbr={ECCV},
  title={Spsn: Superpixel prototype sampling network for rgb-d salient object detection},
  author={Lee, Minhyeok and Park, Chaewon and Cho, Suhwan and Lee, Sangyoun},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIX},
  pages={630--647},
  year={2022},
  organization={Springer},
  arxiv={2207.07898},
  code={https://github.com/Hydragon516/SPSN},
  abstract={RGB-D salient object detection (SOD) has been in the spotlight recently because it is an important preprocessing operation for various vision tasks. However, despite advances in deep learning-based methods, RGB-D SOD is still challenging due to the large domain gap between an RGB image and the depth map and low-quality depth maps. To solve this problem, we propose a novel superpixel prototype sampling network (SPSN) architecture. The proposed model splits the input RGB image and depth map into component superpixels to generate component prototypes. We design a prototype sampling network so that the network only samples prototypes corresponding to salient objects. In addition, we propose a reliance selection module to recognize the quality of each RGB and depth feature map and adaptively weight them in proportion to their reliability. The proposed method makes the model robust to inconsistencies between RGB images and depth maps and eliminates the influence of non-salient objects. Our method is evaluated on five popular datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.}
}

@inproceedings{park2022fastano,
  abbr={WACV},
  title={FastAno: Fast anomaly detection via spatio-temporal patch transformation},
  author={Park, Chaewon and Cho, MyeongAh and Lee, Minhyeok and Lee, Sangyoun},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2249--2259},
  year={2022},
  arxiv={2106.08613},
  abstract={Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.}
}

@article{lee2022hierarchically,
  abbr={arXiv},
  title={Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition},
  author={Lee, Jungho and Lee, Minhyeok and Lee, Dogyoon and Lee, Sangyoon},
  journal={arXiv preprint arXiv:2208.10741},
  year={2022},
  arxiv={2208.10741},
  code={https://github.com/Jho-Yonsei/HD-GCN},
  abstract={Graph convolutional networks (GCNs) are the most commonly used methods for skeleton-based action recognition and have achieved remarkable performance. Generating adjacency matrices with semantically meaningful edges is particularly important for this task, but extracting such edges is challenging problem. To solve this, we propose a hierarchically decomposed graph convolutional network (HD-GCN) architecture with a novel hierarchically decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every joint node into several sets to extract major structurally adjacent and distant edges, and uses them to construct an HD-Graph containing those edges in the same semantic spaces of a human skeleton. In addition, we introduce an attention-guided hierarchy aggregation (A-HA) module to highlight the dominant hierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way ensemble method, which uses only joint and bone stream without any motion stream. The proposed model is evaluated and achieves state-of-the-art performance on three large, popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, and Northwestern-UCLA. Finally, we demonstrate the effectiveness of our model with various comparative experiments.}
}

@inproceedings{lee2022edgeconv,
  abbr={WACV},
  title={Edgeconv with attention module for monocular depth estimation},
  author={Lee, Minhyeok and Hwang, Sangwon and Park, Chaewon and Lee, Sangyoun},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2858--2867},
  year={2022},
  arxiv={2106.08615},
  abstract={Monocular depth estimation is an especially important task in robotics and autonomous driving, where 3D structural information is essential. However, extreme lighting conditions and complex surface objects make it difficult to predict depth in a single image. Therefore, to generate accurate depth maps, it is important for the model to learn structural information about the scene. We propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module (EAM) to solve the difficulty of monocular depth estimation. The proposed modules extract structural information by learning the relationship between image patches close to each other in space using edge convolution. Our method is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen split, achieving state-of-the-art performance. We prove that the proposed model predicts depth robustly in challenging scenes through various comparative experiments.}
}

@inproceedings{cho2023treating,
  title={Treating motion as option to reduce motion dependency in unsupervised video object segmentation},
  author={Cho, Suhwan and Lee, Minhyeok and Lee, Seunghoon and Park, Chaewon and Kim, Donghyeong and Lee, Sangyoun},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5140--5149},
  year={2023}
}

@inproceedings{lee2023unsupervised,
  title={Unsupervised Video Object Segmentation via Prototype Memory Network},
  author={Lee, Minhyeok and Cho, Suhwan and Lee, Seunghoon and Park, Chaewon and Lee, Sangyoun},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5924--5934},
  year={2023}
}

@inproceedings{cho2022tackling,
  title={Tackling background distraction in video object segmentation},
  author={Cho, Suhwan and Lee, Heansung and Lee, Minhyeok and Park, Chaewon and Jang, Sungjun and Kim, Minjung and Lee, Sangyoun},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXII},
  pages={446--462},
  year={2022},
  organization={Springer}
}

@inproceedings{park2022saliency,
  title={Saliency detection via global context enhanced feature fusion and edge weighted loss},
  author={Park, Chaewon and Lee, Minhyeok and Cho, MyeongAh and Lee, Sangyoun},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},
  pages={811--815},
  year={2022},
  organization={IEEE}
}

@inproceedings{lee2022superpixel,
  title={Superpixel Group-Correlation Network for Co-Saliency Detection},
  author={Lee, Minhyeok and Park, Chaewon and Cho, Suhwan and Lee, Sangyoun},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},
  pages={806--810},
  year={2022},
  organization={IEEE}
}

@article{park2022randomsemo,
  title={RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly Detection},
  author={Park, Chaewon and Lee, Minhyeok and Cho, MyeongAh and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2202.06256},
  year={2022}
}

@article{lee2023adaptive,
  title={Adaptive Graph Convolution Module for Salient Object Detection},
  author={Lee, Yongwoo and Lee, Minhyeok and Cho, Suhwan and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2303.09801},
  year={2023}
}

@article{lee2023guided,
  title={Guided Slot Attention for Unsupervised Video Object Segmentation},
  author={Lee, Minhyeok and Cho, Suhwan and Lee, Dogyoon and Park, Chaewon and Lee, Jungho and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2303.08314},
  year={2023}
}

@article{lee2023tsanet,
  title={TSANET: Temporal and Scale Alignment for Unsupervised Video Object Segmentation},
  author={Lee, Seunghoon and Cho, Suhwan and Lee, Dogyoon and Lee, Minhyeok and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2303.04376},
  year={2023}
}

@article{park2023two,
  title={Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection},
  author={Park, Chaewon and Lee, Minhyeok and Cho, Suhwan and Kim, Donghyeong and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2302.09794},
  year={2023}
}

@article{lee2022leveraging,
  title={Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition},
  author={Lee, Jungho and Lee, Minhyeok and Cho, Suhwan and Woo, Sungmin and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2212.04761},
  year={2022}
}

@article{lee2022deblurred,
  title={Deblurred Neural Radiance Field with Physical Scene Priors},
  author={Lee, Dogyoon and Lee, Minhyeok and Shin, Chajin and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2211.12046},
  year={2022}
}

@article{cho2022domain,
  title={Domain Alignment and Temporal Aggregation for Unsupervised Video Object Segmentation},
  author={Cho, Suhwan and Lee, Minhyeok and Lee, Seunghoon and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2211.12036},
  year={2022}
}

@article{lee2022global,
  title={Boundary-aware Camouflaged Object Detection via Deformable Point Sampling},
  author={Lee, Minhyeok and Cho, Suhwan and Park, Chaewon and Lee, Dogyoon and Lee, Jungho and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2211.12048},
  year={2022}
}

@article{cho2022pixel,
  title={Pixel-Level Equalized Matching for Video Object Segmentation},
  author={Cho, Suhwan and Kim, Woo Jin and Cho, MyeongAh and Lee, Seunghoon and Lee, Minhyeok and Park, Chaewon and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2209.03139},
  year={2022}
}

@inproceedings{lee2021multi,
  title={Multi-level Feature Maps Attention for Monocular Depth Estimation},
  author={Lee, Seunghoon and Lee, Minhyeok and Lee, Sangyoon},
  booktitle={2021 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)},
  pages={1--4},
  year={2021},
  organization={IEEE}
}

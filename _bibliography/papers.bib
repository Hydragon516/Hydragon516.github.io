@inproceedings{lee2021regularization,
  selected={true},
  abbr={CVPR},
  title={Regularization strategy for point cloud via rigidly mixed sample},
  author={Lee, Dogyoon and Lee, Jaeha and Lee, Junhyeop and Lee, Hyeongmin and Lee, Minhyeok and Woo, Sungmin and Lee, Sangyoun},
  booktitle={[CVPR 2021] Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15900--15909},
  year={2021},
  arxiv={2102.01929},
  code={https://github.com/dogyoonlee/RSMix},
  abstract={Data augmentation is an effective regularization strategy to alleviate the overfitting, which is an inherent drawback of the deep neural networks. However, data augmentation is rarely considered for point cloud processing despite many studies proposing various augmentation methods for image data. Actually, regularization is essential for point clouds since lack of generality is more likely to occur in point cloud due to small datasets. This paper proposes a Rigid Subset Mix (RSMix), a novel data augmentation method for point clouds that generates a virtual mixed sample by replacing part of the sample with shape-preserved subsets from another sample. RSMix preserves structural information of the point cloud sample by extracting subsets from each sample without deformation using a neighboring function. The neighboring function was carefully designed considering unique properties of point cloud, unordered structure and non-grid. Experiments verified that RSMix successfully regularized the deep neural networks with remarkable improvement for shape classification. We also analyzed various combinations of data augmentations including RSMix with single and multi-view evaluations, based on abundant ablation studies.},
  preview={rsmix.png}
}

@inproceedings{lee2022robust,
  selected={true},
  abbr={WACV},
  title={Robust lane detection via expanded self attention},
  author={Lee, Minhyeok and Lee, Junhyeop and Lee, Dogyoon and Kim, Woojin and Hwang, Sangwon and Lee, Sangyoun},
  booktitle={[WACV 2022] Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={533--542},
  year={2022},
  arxiv={2102.07037},
  code={https://github.com/Hydragon516/ESA-official},
  abstract={The image-based lane detection algorithm is one of the key technologies in autonomous vehicles. Modern deep learning methods achieve high performance in lane detection, but it is still difficult to accurately detect lanes in challenging situations such as congested roads and extreme lighting conditions. To be robust on these challenging situations, it is important to extract global contextual information even from limited visual cues. In this paper, we propose a simple but powerful self-attention mechanism optimized for lane detection called the Expanded Self Attention (ESA) module. Inspired by the simple geometric structure of lanes, the proposed method predicts the confidence of a lane along the vertical and horizontal directions in an image. The prediction of the confidence enables estimating occluded locations by extracting global contextual information. ESA module can be easily implemented and applied to any encoder-decoder-based model without increasing the inference time. The performance of our method is evaluated on three popular lane detection benchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art performance in CULane and BDD100K and distinct improvement on TuSimple dataset. The experimental results show that our approach is robust to occlusion and extreme lighting conditions.},
  preview={ESA.gif}
}

@inproceedings{lee2023unsupervised,
  selected={true},
  abbr={WACV},
  title={Unsupervised Video Object Segmentation via Prototype Memory Network},
  author={Lee, Minhyeok and Cho, Suhwan and Lee, Seunghoon and Park, Chaewon and Lee, Sangyoun},
  booktitle={[WACV 2023] Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5924--5934},
  year={2023},
  arxiv={2209.03712},
  code={https://github.com/Hydragon516/PMN},
  abstract={Unsupervised video object segmentation aims to segment a target object in the video without a ground truth mask in the initial frame. This challenging task requires extracting features for the most salient common objects within a video sequence. This difficulty can be solved by using motion information such as optical flow, but using only the information between adjacent frames results in poor connectivity between distant frames and poor performance. To solve this problem, we propose a novel prototype memory network architecture. The proposed model effectively extracts the RGB and motion information by extracting superpixel-based component prototypes from the input RGB images and optical flow maps. In addition, the model scores the usefulness of the component prototypes in each frame based on a self-learning algorithm and adaptively stores the most useful prototypes in memory and discards obsolete prototypes. We use the prototypes in the memory bank to predict the next query frames mask, which enhances the association between distant frames to help with accurate mask prediction. Our method is evaluated on three datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed model with various ablation studies.},
  preview={PMN.png}
}

@inproceedings{cho2022tackling,
  selected={true},
  abbr={ECCV},
  title={Tackling background distraction in video object segmentation},
  author={Cho, Suhwan and Lee, Heansung and Lee, Minhyeok and Park, Chaewon and Jang, Sungjun and Kim, Minjung and Lee, Sangyoun},
  booktitle={[ECCV 2022] Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXII},
  pages={446--462},
  year={2022},
  organization={Springer},
  arxiv={2207.06953},
  code={https://github.com/suhwan-cho/TBD},
  abstract={Semi-supervised video object segmentation (VOS) aims to densely track certain designated objects in videos. One of the main challenges in this task is the existence of background distractors that appear similar to the target objects. We propose three novel strategies to suppress such distractors: 1) a spatio-temporally diversified template construction scheme to obtain generalized properties of the target objects; 2) a learnable distance-scoring function to exclude spatially-distant distractors by exploiting the temporal consistency between two consecutive frames; 3) swap-and-attach augmentation to force each object to have unique features by providing training samples containing entangled objects. On all public benchmark datasets, our model achieves a comparable performance to contemporary state-of-the-art approaches, even with real-time performance. Qualitative results also demonstrate the superiority of our approach over existing methods. We believe our approach will be widely used for future VOS research.},
  preview={TBD.png}
}

@inproceedings{park2022saliency,
  addr={ICIP},
  title={Saliency detection via global context enhanced feature fusion and edge weighted loss},
  author={Park, Chaewon and Lee, Minhyeok and Cho, MyeongAh and Lee, Sangyoun},
  booktitle={[ICIP 2022] 2022 IEEE International Conference on Image Processing},
  pages={811--815},
  year={2022},
  organization={IEEE},
  arxiv={2110.06550},
  abstract={UNet-based methods have shown outstanding performance in salient object detection (SOD), but are problematic in two aspects. 1) Indiscriminately integrating the encoder feature, which contains spatial information for multiple objects, and the decoder feature, which contains global information of the salient object, is likely to convey unnecessary details of non-salient objects to the decoder, hindering saliency detection. 2) To deal with ambiguous object boundaries and generate accurate saliency maps, the model needs additional branches, such as edge reconstructions, which leads to increasing computational cost. To address the problems, we propose a context fusion decoder network (CFDN) and near edge weighted loss (NEWLoss) function. The CFDN creates an accurate saliency map by integrating global context information and thus suppressing the influence of the unnecessary spatial information. NEWLoss accelerates learning of obscure boundaries without additional modules by generating weight maps on object boundaries. Our method is evaluated on four benchmarks and achieves state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.},
  preview={CFDN.png}
}

@inproceedings{lee2022superpixel,
  addr={ICIP},
  title={Superpixel Group-Correlation Network for Co-Saliency Detection},
  author={Lee, Minhyeok and Park, Chaewon and Cho, Suhwan and Lee, Sangyoun},
  booktitle={[ICIP 2022] 2022 IEEE International Conference on Image Processing},
  pages={806--810},
  year={2022},
  organization={IEEE},
  abstract={Co-saliency detection is a task to segment the occurring salient objects in a group of images. The biggest challenges are distracting objects in the background and ambiguity between the foreground and background. To handle these issues, we propose a novel superpixel group-correlation network (SGCN) architecture that uses a superpixel algorithm to obtain various component features from a group of images and creates a group-correlation matrix to detect the common components of those images. In this way, non-common objects can be effectively excluded from consideration, enabling a clear distinction between foreground and background. Our method outperforms current state-of-the-art methods on three popular benchmark datasets for co-saliency detection, and our extensive experiments thoroughly validate our claimed contributions.},
  preview={SGCN.png}
}

@article{park2022randomsemo,
  addr={arXiv},
  title={RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly Detection},
  author={Park, Chaewon and Lee, Minhyeok and Cho, MyeongAh and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2202.06256},
  year={2022},
  arxiv={2202.06256},
  abstract={Recent anomaly detection algorithms have shown powerful performance by adopting frame predicting autoencoders. However, these methods face two challenging circumstances. First, they are likely to be trained to be excessively powerful, generating even abnormal frames well, which leads to failure in detecting anomalies. Second, they are distracted by the large number of objects captured in both foreground and background. To solve these problems, we propose a novel superpixel-based video data transformation technique named Random Superpixel Erasing on Moving Objects (RandomSEMO) and Moving Object Loss (MOLoss), built on top of a simple lightweight autoencoder. RandomSEMO is applied to the moving object regions by randomly erasing their superpixels. It enforces the network to pay attention to the foreground objects and learn the normal features more effectively, rather than simply predicting the future frame. Moreover, MOLoss urges the model to focus on learning normal objects captured within RandomSEMO by amplifying the loss on the pixels near the moving objects. The experimental results show that our model outperforms state-of-the-arts on three benchmarks.},
  preview={RandomSEMO.png}
}

@article{lee2023adaptive,
  addr={ICIP},
  title={Adaptive Graph Convolution Module for Salient Object Detection},
  author={Lee, Yongwoo and Lee, Minhyeok and Cho, Suhwan and Lee, Sangyoun},
  journal={[ICIP 2023] 2023 IEEE International Conference on Image Processing},
  year={2023},
  arxiv={2303.09801},
  abstract={Salient object detection (SOD) is a task that involves identifying and segmenting the most visually prominent object in an image. Existing solutions can accomplish this use a multi-scale feature fusion mechanism to detect the global context of an image. However, as there is no consideration of the structures in the image nor the relations between distant pixels, conventional methods cannot deal with complex scenes effectively. In this paper, we propose an adaptive graph convolution module (AGCM) to overcome these limitations. Prototype features are initially extracted from the input image using a learnable region generation layer that spatially groups features in the image. The prototype features are then refined by propagating information between them based on a graph architecture, where each feature is regarded as a node. Experimental results show that the proposed AGCM dramatically improves the SOD performance both quantitatively and quantitatively.},
  preview={adagcn.png}
}

@article{lee2023guided,
  addr={arXiv},
  title={Guided Slot Attention for Unsupervised Video Object Segmentation},
  author={Lee, Minhyeok and Cho, Suhwan and Lee, Dogyoon and Park, Chaewon and Lee, Jungho and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2303.08314},
  year={2023},
  arxiv={2303.08314},
  abstract={Unsupervised video object segmentation aims to segment the most prominent object in a video sequence. However, the existence of complex backgrounds and multiple foreground objects make this task challenging. To address this issue, we propose a guided slot attention network to reinforce spatial structural information and obtain better foreground--background separation. The foreground and background slots, which are initialized with query guidance, are iteratively refined based on interactions with template information. Furthermore, to improve slot--template interaction and effectively fuse global and local features in the target and reference frames, K-nearest neighbors filtering and a feature aggregation transformer are introduced. The proposed model achieves state-of-the-art performance on two popular datasets. Additionally, we demonstrate the robustness of the proposed model in challenging scenes through various comparative experiments.},
  preview={GSA-Net.gif}
}

@inproceedings{lee2022spsn,
  selected={true},
  abbr={ECCV},
  title={Spsn: Superpixel prototype sampling network for rgb-d salient object detection},
  author={Lee, Minhyeok and Park, Chaewon and Cho, Suhwan and Lee, Sangyoun},
  booktitle={[ECCV 2022] Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIX},
  pages={630--647},
  year={2022},
  organization={Springer},
  arxiv={2207.07898},
  code={https://github.com/Hydragon516/SPSN},
  abstract={RGB-D salient object detection (SOD) has been in the spotlight recently because it is an important preprocessing operation for various vision tasks. However, despite advances in deep learning-based methods, RGB-D SOD is still challenging due to the large domain gap between an RGB image and the depth map and low-quality depth maps. To solve this problem, we propose a novel superpixel prototype sampling network (SPSN) architecture. The proposed model splits the input RGB image and depth map into component superpixels to generate component prototypes. We design a prototype sampling network so that the network only samples prototypes corresponding to salient objects. In addition, we propose a reliance selection module to recognize the quality of each RGB and depth feature map and adaptively weight them in proportion to their reliability. The proposed method makes the model robust to inconsistencies between RGB images and depth maps and eliminates the influence of non-salient objects. Our method is evaluated on five popular datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.},
  preview={SPSN.png}
}

@inproceedings{park2022fastano,
  selected={true},
  abbr={WACV},
  title={FastAno: Fast anomaly detection via spatio-temporal patch transformation},
  author={Park, Chaewon and Cho, MyeongAh and Lee, Minhyeok and Lee, Sangyoun},
  booktitle={[WACV 2022] Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2249--2259},
  year={2022},
  arxiv={2106.08613},
  abstract={Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.},
  preview={FastAno.png}
}

@inproceedings{lee2023hierarchically,
  selected={true},
  abbr={ICCV},
  title={Hierarchically decomposed graph convolutional networks for skeleton-based action recognition},
  author={Lee, Jungho and Lee, Minhyeok and Lee, Dogyoon and Lee, Sangyoun},
  booktitle={[ICCV 2023] Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10444--10453},
  year={2023},
  arxiv={2208.10741},
  code={https://github.com/Jho-Yonsei/HD-GCN},
  abstract={Graph convolutional networks (GCNs) are the most commonly used methods for skeleton-based action recognition and have achieved remarkable performance. Generating adjacency matrices with semantically meaningful edges is particularly important for this task, but extracting such edges is challenging problem. To solve this, we propose a hierarchically decomposed graph convolutional network (HD-GCN) architecture with a novel hierarchically decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every joint node into several sets to extract major structurally adjacent and distant edges, and uses them to construct an HD-Graph containing those edges in the same semantic spaces of a human skeleton. In addition, we introduce an attention-guided hierarchy aggregation (A-HA) module to highlight the dominant hierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way ensemble method, which uses only joint and bone stream without any motion stream. The proposed model is evaluated and achieves state-of-the-art performance on three large, popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, and Northwestern-UCLA. Finally, we demonstrate the effectiveness of our model with various comparative experiments.},
  preview={HD-GCN.gif}
}

@inproceedings{lee2022edgeconv,
  selected={true},
  abbr={WACV},
  title={Edgeconv with attention module for monocular depth estimation},
  author={Lee, Minhyeok and Hwang, Sangwon and Park, Chaewon and Lee, Sangyoun},
  booktitle={[WACV 2022] Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2858--2867},
  year={2022},
  arxiv={2106.08615},
  abstract={Monocular depth estimation is an especially important task in robotics and autonomous driving, where 3D structural information is essential. However, extreme lighting conditions and complex surface objects make it difficult to predict depth in a single image. Therefore, to generate accurate depth maps, it is important for the model to learn structural information about the scene. We propose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module (EAM) to solve the difficulty of monocular depth estimation. The proposed modules extract structural information by learning the relationship between image patches close to each other in space using edge convolution. Our method is evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen split, achieving state-of-the-art performance. We prove that the proposed model predicts depth robustly in challenging scenes through various comparative experiments.},
  preview={Edgeconv.png}
}

@inproceedings{cho2023treating,
  selected={true},
  abbr={WACV},
  title={Treating motion as option to reduce motion dependency in unsupervised video object segmentation},
  author={Cho, Suhwan and Lee, Minhyeok and Lee, Seunghoon and Park, Chaewon and Kim, Donghyeong and Lee, Sangyoun},
  booktitle={[WACV 2023] Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5140--5149},
  year={2023},
  arxiv={2209.03138},
  code={https://github.com/suhwan-cho/tmo},
  abstract={Unsupervised video object segmentation (VOS) aims to detect the most salient object in a video sequence at the pixel level. In unsupervised VOS, most state-of-the-art methods leverage motion cues obtained from optical flow maps in addition to appearance cues to exploit the property that salient objects usually have distinctive movements compared to the background. However, as they are overly dependent on motion cues, which may be unreliable in some cases, they cannot achieve stable prediction. To reduce this motion dependency of existing two-stream VOS methods, we propose a novel motion-as-option network that optionally utilizes motion cues. Additionally, to fully exploit the property of the proposed network that motion is not always required, we introduce a collaborative network learning strategy. On all the public benchmark datasets, our proposed network affords state-of-the-art performance with real-time inference speed.},
  preview={TMO.png}
}

@article{lee2023tsanet,
  addr={ICIP},
  title={TSANET: Temporal and Scale Alignment for Unsupervised Video Object Segmentation},
  author={Lee, Seunghoon and Cho, Suhwan and Lee, Dogyoon and Lee, Minhyeok and Lee, Sangyoun},
  journal={[ICIP 2023] 2023 IEEE International Conference on Image Processing},
  year={2023},
  arxiv={2303.04376},
  abstract={Unsupervised Video Object Segmentation (UVOS) refers to the challenging task of segmenting the prominent object in videos without manual guidance. In other words, the network detects the accurate region of the target object in a sequence of RGB frames without prior knowledge. In recent works, two approaches for UVOS have been discussed that can be divided into: appearance and appearance-motion based methods. Appearance based methods utilize the correlation information of inter-frames to capture target object that commonly appears in a sequence. However, these methods does not consider the motion of target object due to exploit the correlation information between randomly paired frames. Appearance-motion based methods, on the other hand, fuse the appearance features from RGB frames with the motion features from optical flow. Motion cue provides useful information since salient objects typically show distinctive motion in a sequence. However, these approaches have the limitation that the dependency on optical flow is dominant. In this paper, we propose a novel framework for UVOS that can address aforementioned limitations of two approaches in terms of both time and scale. Temporal Alignment Fusion aligns the saliency information of adjacent frames with the target frame to leverage the information of adjacent frames. Scale Alignment Decoder predicts the target object mask precisely by aggregating differently scaled feature maps via continuous mapping with implicit neural representation. We present experimental results on public benchmark datasets, DAVIS 2016 and FBMS, which demonstrate the effectiveness of our method. Furthermore, we outperform the state-of-the-art methods on DAVIS 2016.},
  preview={TSANET.png}  
}

@article{park2023two,
  addr={ICASSP},
  title={Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection},
  author={Park, Chaewon and Lee, Minhyeok and Cho, Suhwan and Kim, Donghyeong and Lee, Sangyoun},
  journal={[ICASSP 2023] IEEE International Conference on Acoustics, Speech and Signal Processing 2023},
  year={2023},
  arxiv={2302.09794},
  abstract={Image reconstruction-based anomaly detection has recently been in the spotlight because of the difficulty of constructing anomaly datasets. These approaches work by learning to model normal features without seeing abnormal samples during training and then discriminating anomalies at test time based on the reconstructive errors. However, these models have limitations in reconstructing the abnormal samples due to their indiscriminate conveyance of features. Moreover, these approaches are not explicitly optimized for distinguishable anomalies. To address these problems, we propose a two-stream decoder network (TSDN), designed to learn both normal and abnormal features. Additionally, we propose a feature normality estimator (FNE) to eliminate abnormal features and prevent high-quality reconstruction of abnormal regions. Evaluation on a standard benchmark demonstrated performance better than state-of-the-art models.},
  preview={TSDN.png}
}

@inproceedings{lee2023leveraging,
  addr={ICCV},
  title={Leveraging spatio-temporal dependency for skeleton-based action recognition},
  author={Lee, Jungho and Lee, Minhyeok and Cho, Suhwan and Woo, Sungmin and Jang, Sungjun and Lee, Sangyoun},
  booktitle={[ICCV 2023] Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10255--10264},
  year={2023},
  arxiv={2212.04761},
  abstract={Skeleton-based action recognition has attracted considerable attention due to its compact skeletal structure of the human body. Many recent methods have achieved remarkable performance using graph convolutional networks (GCNs) and convolutional neural networks (CNNs), which extract spatial and temporal features, respectively. Although spatial and temporal dependencies in the human skeleton have been explored, spatio-temporal dependency is rarely considered. In this paper, we propose the Inter-Frame Curve Network (IFC-Net) to effectively leverage the spatio-temporal dependency of the human skeleton. Our proposed network consists of two novel elements: 1) The Inter-Frame Curve (IFC) module; and 2) Dilated Graph Convolution (D-GC). The IFC module increases the spatio-temporal receptive field by identifying meaningful node connections between every adjacent frame and generating spatio-temporal curves based on the identified node connections. The D-GC allows the network to have a large spatial receptive field, which specifically focuses on the spatial domain. The kernels of D-GC are computed from the given adjacency matrices of the graph and reflect large receptive field in a way similar to the dilated CNNs. Our IFC-Net combines these two modules and achieves state-of-the-art performance on three skeleton-based action recognition benchmarks: NTU-RGB+D 60, NTU-RGB+D 120, and Northwestern-UCLA.},
  preview={STCNet.gif}
}

@inproceedings{Lee_2023_CVPR,
  selected={true},
  abbr={CVPR},
  title={DP-NeRF: Deblurred Neural Radiance Field With Physical Scene Priors},
  author={Lee, Dogyoon and Lee, Minhyeok and Shin, Chajin and Lee, Sangyoun},
  booktitle={[CVPR 2023] Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12386-12396},
  year={2023},
  arxiv={2211.12046},
  code={https://github.com/dogyoonlee/DP-NeRF},
  abstract={Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D) reconstruction quality via the novel view synthesis from multi-view images and paired calibrated camera parameters. However, previous NeRF-based systems have been demonstrated under strictly controlled settings, with little attention paid to less ideal scenarios, including with the presence of noise such as exposure, illumination changes, and blur. In particular, though blur frequently occurs in real situations, NeRF that can handle blurred images has received little attention. The few studies that have investigated NeRF for blurred images have not considered geometric and appearance consistency in 3D space, which is one of the most important factors in 3D reconstruction. This leads to inconsistency and the degradation of the perceptual quality of the constructed scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for blurred images, which is constrained with two physical priors. These priors are derived from the actual blurring process during image acquisition by the camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency utilizing the physical priors and adaptive weight proposal to refine the color composition error in consideration of the relationship between depth and blur. We present extensive experimental results for synthetic and real scenes with two types of blur: camera motion blur and defocus blur. The results demonstrate that DP-NeRF successfully improves the perceptual quality of the constructed NeRF ensuring 3D geometric and appearance consistency. We further demonstrate the effectiveness of our model with comprehensive ablation analysis.},
  preview={DP-Nerf.gif}
}

@article{cho2022domain,
  addr={arXiv},
  title={Dual Prototype Attention for Unsupervised Video Object Segmentation},
  author={Cho, Suhwan and Lee, Minhyeok and Lee, Seunghoon and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2211.12036},
  year={2022},
  arxiv={2211.12036},
  code={https://github.com/Hydragon516/DPA},
  abstract={Unsupervised video object segmentation (VOS) aims to detect and segment the most salient object in videos. The primary techniques used in unsupervised VOS are 1) the collaboration of appearance and motion information and 2) temporal fusion between different frames. This paper proposes two novel prototype-based attention mechanisms, inter-modality attention (IMA) and inter-frame attention (IFA), to incorporate these techniques via dense propagation across different modalities and frames. IMA densely integrates context information from different modalities based on a mutual refinement. IFA injects global context of a video to the query frame, enabling a full utilization of useful properties from multiple frames. Experimental results on public benchmark datasets demonstrate that our proposed approach outperforms all existing methods by a substantial margin. The proposed two components are also thoroughly validated via ablative study.},
  preview={DPA.png}
}

@article{lee2022global,
  addr={arXiv},
  title={Boundary-aware Camouflaged Object Detection via Deformable Point Sampling},
  author={Lee, Minhyeok and Cho, Suhwan and Park, Chaewon and Lee, Dogyoon and Lee, Jungho and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2211.12048},
  year={2022},
  arxiv={2211.12048},
  abstract={The camouflaged object detection (COD) task aims to identify and segment objects that blend into the background due to their similar color or texture. Despite the inherent difficulties of the task, COD has gained considerable attention in several fields, such as medicine, life-saving, and anti-military fields. In this paper, we propose a novel solution called the Deformable Point Sampling network (DPS-Net) to address the challenges associated with COD. The proposed DPS-Net utilizes a Deformable Point Sampling transformer (DPS transformer) that can effectively capture sparse local boundary information of significant object boundaries in COD using a deformable point sampling method. Moreover, the DPS transformer demonstrates robust COD performance by extracting contextual features for target object localization through integrating rough global positional information of objects with boundary local information. We evaluate our method on three prominent datasets and achieve state-of-the-art performance. Our results demonstrate the effectiveness of the proposed method through comparative experiments.},
  preview={DPS-Net.gif}
}

@article{cho2022pixel,
  addr={arXiv},
  title={Pixel-Level Equalized Matching for Video Object Segmentation},
  author={Cho, Suhwan and Kim, Woo Jin and Cho, MyeongAh and Lee, Seunghoon and Lee, Minhyeok and Park, Chaewon and Lee, Sangyoun},
  journal={arXiv preprint arXiv:2209.03139},
  year={2022},
  arxiv={2209.03139},
  abstract={Feature similarity matching, which transfers the information of the reference frame to the query frame, is a key component in semi-supervised video object segmentation. If surjective matching is adopted, background distractors can easily occur and degrade the performance. Bijective matching mechanisms try to prevent this by restricting the amount of information being transferred to the query frame, but have two limitations: 1) surjective matching cannot be fully leveraged as it is transformed to bijective matching at test time; and 2) test-time manual tuning is required for searching the optimal hyper-parameters. To overcome these limitations while ensuring reliable information transfer, we introduce an equalized matching mechanism. To prevent the reference frame information from being overly referenced, the potential contribution to the query frame is equalized by simply applying a softmax operation along with the query. On public benchmark datasets, our proposed approach achieves a comparable performance to state-of-the-art methods.},
  preview={EMVOS.png}
}

@inproceedings{lee2021multi,
  addr={ICCE-Asia},
  title={Multi-level Feature Maps Attention for Monocular Depth Estimation},
  author={Lee, Seunghoon and Lee, Minhyeok and Lee, Sangyoon},
  booktitle={2021 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)},
  pages={1--4},
  year={2021},
  organization={IEEE},
  abstract={[ICCE-Asia 2021] Monocular depth estimation is a fundamental task in autonomous driving, robotics, virtual reality. Monocular depth estimation is attracting research due to the efficiency of predicting depth map from a single RGB image. However, Monocular depth estimation is an ill-posed problem and is sensitive to image compositions such as light condition, occlusion, noise. We propose an encoder-decoder based network that uses multi-level attention and aggregate densely weighted feature map. Our model is evaluated on NYU Depth v2. Experimental results demonstrated that our model achieves promising performance.},
  preview={MFMA.png}
}
